from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
import os
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List
from langchain_community.embeddings import DashScopeEmbeddings

from langchain_openai import ChatOpenAI

load_dotenv()

def create_llm(temperature=0, model_name=None, provider=None):
    """
    åˆ›å»º LLM å®ä¾‹çš„å·¥å…·å‡½æ•°

    Args:
        temperature (float): æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œé»˜è®¤ä¸º 0
        model_name (str): æ¨¡å‹åç§°ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä» .env æ–‡ä»¶è¯»å–
        max_tokens (int): æœ€å¤§ token æ•°ï¼Œé»˜è®¤ä¸º 4000
        provider (str): æ¨¡å‹æä¾›å•†ï¼Œæ”¯æŒ "openai", "qwen", "doubao"ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä» .env æ–‡ä»¶è¯»å–

    Returns:
        ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # åŠ è½½ç¯å¢ƒå˜é‡
    if provider is None:
        provider = os.getenv('DEFAULT_SERVICE', 'QWEN')
        provider = provider.upper()

    # å¦‚æœ model_name ä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
    if model_name is None:
        model_name = os.getenv('LLM_MODEL_NAME', 'gpt-4o')

    # ä½¿ç”¨ match-case å®ç° switch case åŠŸèƒ½
    match provider.lower():
        case "openai":
            api_key = os.getenv('OPENAI_API_KEY')
            base_url = os.getenv('OPENAI_API_BASE', None)


        case "qwen":
            api_key = os.getenv('QWEN_API_KEY')
            model_name = os.getenv('QWEN_MODEL', 'qwen-max')
            base_url = os.getenv('QWEN_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')


        case "doubao":
            api_key = os.getenv('VOLC_API_KEY')
            model_name = os.getenv('VOLC_MODEL_NAME', 'doubao-1-5-lite-32k-250115')
            base_url = os.getenv('VOLC_BASE_URL', 'https://api.doubao.com/v1')

        case _:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ã€‚æ”¯æŒçš„æä¾›å•†: openai, qwen, doubao")

    if not api_key:
        raise ValueError(f"è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®äº† {provider}_API_KEY")

    # ä½¿ç”¨ç»Ÿä¸€çš„ ChatOpenAI æ„é€ æ–¹æ³•
    return ChatOpenAI(
        model=model_name,
        base_url=base_url,
        api_key=api_key,
        temperature=temperature
    )


# é¢„å®šä¹‰çš„å“åº”æ¨¡å¼
class SerpQueriesResponse(BaseModel):
    """SERP æŸ¥è¯¢å“åº”æ¨¡å¼"""
    queries: List[Dict[str, str]] = Field(..., description="æœç´¢æŸ¥è¯¢åˆ—è¡¨")


class LearningsResponse(BaseModel):
    """å­¦ä¹ è¦ç‚¹å“åº”æ¨¡å¼"""
    learnings: List[str] = Field(..., description="å­¦ä¹ è¦ç‚¹åˆ—è¡¨")
    summary: List[str] = Field(..., description="æ‘˜è¦åˆ—è¡¨")
    followUpQuestions: List[str] = Field(..., description="åç»­é—®é¢˜åˆ—è¡¨")


class ReportResponse(BaseModel):
    """æŠ¥å‘Šå“åº”æ¨¡å¼"""
    reportMarkdown: str = Field(..., description="Markdown æ ¼å¼çš„æŠ¥å‘Šå†…å®¹")


async def get_langchain_response(
    system_prompt: str,
    user_prompt: str,
    response_schema: Optional[BaseModel] = None,
    service_provider_name: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LangChain è·å– AI å“åº”

    Args:
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_prompt: ç”¨æˆ·æç¤ºè¯
        response_schema: å“åº”æ¨¡å¼ï¼ˆPydantic æ¨¡å‹ï¼‰
        service_provider_name: æœåŠ¡æä¾›å•†åç§°

    Returns:
        Dict[str, Any]: è§£æåçš„å“åº”
    """
    # åˆ›å»º LLM å®ä¾‹
    llm = create_llm(provider=service_provider_name)

    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    if response_schema:
        # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
        structured_llm = llm.with_structured_output(response_schema)
        result = await structured_llm.ainvoke(messages)
        return result.dict()
    else:
        # ä½¿ç”¨ JSON è¾“å‡ºè§£æå™¨
        json_parser = JsonOutputParser()
        chain = llm | json_parser
        result = await chain.ainvoke(messages)
        return result


def create_embeddings(provider=None, model_name=None):
    """
    åˆ›å»º Embeddings å®ä¾‹çš„å·¥å…·å‡½æ•°

    Args:
        provider (str): æ¨¡å‹æä¾›å•†ï¼Œæ”¯æŒ "openai", "qwen", "doubao"ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä» .env æ–‡ä»¶è¯»å–
        model_name (str): æ¨¡å‹åç§°ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹

    Returns:
        OpenAIEmbeddings: é…ç½®å¥½çš„ Embeddings å®ä¾‹
    """
    # åŠ è½½ç¯å¢ƒå˜é‡

    # å¦‚æœ provider ä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–
    if provider is None:
        provider = os.getenv('DEFAULT_SERVICE', 'QWEN')
        provider = provider.upper()

    print(f"ğŸ”§ åˆ›å»º Embeddings - æä¾›å•†: {provider}")

    # ä½¿ç”¨ match-case å®ç° switch case åŠŸèƒ½
    match provider.lower():
        case "openai":
            api_key = os.getenv('OPENAI_API_KEY')
            base_url = os.getenv('OPENAI_API_BASE', None)
            if model_name is None:
                model_name = "text-embedding-3-small"

        case "qwen":
            api_key = os.getenv('QWEN_API_KEY')
            if model_name is None:
                model_name = "text-embedding-v1"
            base_url = os.getenv('QWEN_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
            # Qwen éœ€è¦ç‰¹æ®Šçš„é…ç½®
            print(f"ğŸ”§ Qwen é…ç½® - ä½¿ç”¨å…¼å®¹æ¨¡å¼")

        # case "doubao":
        #     api_key = os.getenv('VOLC_API_KEY')
        #     if model_name is None:
        #         model_name = "doubao-embedding-vision-250615"
        #     base_url = os.getenv('VOLC_BASE_EMBEDDING_URL', 'https://ark.cn-beijing.volces.com/api/v3/embeddings/multimodal')

        case _:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ã€‚æ”¯æŒçš„æä¾›å•†: openai, qwen")

    if not api_key:
        raise ValueError(f"è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®äº† {provider}_API_KEY")

    print(f"ğŸ”§ Embeddings é…ç½® - æ¨¡å‹: {model_name}, Base URL: {base_url}")

    # å¯¹äº Qwenï¼Œä½¿ç”¨ç‰¹æ®Šçš„åµŒå…¥ç±»
    if provider.lower() == "qwen":

        # embeddings = OpenAIEmbeddings(
        #     model=model_name,
        #     base_url=base_url,
        #     api_key=api_key
        # )
        embeddings = DashScopeEmbeddings(
            model=model_name,  # æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©æ¨¡å‹ç‰ˆæœ¬
            dashscope_api_key=api_key  # æ›¿æ¢ä¸ºä½ çš„é€šä¹‰åƒé—®APIå¯†é’¥
        )
    else:
        # åˆ›å»ºæ ‡å‡†çš„ OpenAIEmbeddings å®ä¾‹
        embeddings = create_embeddings()
    return embeddings


if __name__ == "__main__":
    llm = create_llm()
    print(llm.invoke("ä½ å¥½"))

    # æµ‹è¯• embeddings
    embeddings = create_embeddings()
    print("Embeddings æ¨¡å‹åˆ›å»ºæˆåŠŸ")

    # Chroma å‘é‡å­˜å‚¨æµ‹è¯•
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document
    print("=== Chroma å‘é‡å­˜å‚¨æµ‹è¯• ===")
    docs = [
        Document(page_content="Harry Potter is a wizard.", metadata={"source": "test1"}),
        Document(page_content="Hermione Granger is very smart.", metadata={"source": "test2"}),
        Document(page_content="Ron Weasley is Harry's friend.", metadata={"source": "test3"}),
    ]
    try:
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            collection_name="test_collection"
        )
        print("Chroma å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼")
        # ç®€å•æ£€ç´¢
        retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
        # æ–°å†™æ³•ï¼ˆæ— è­¦å‘Šï¼‰
        results = retriever.invoke("Who is Harry's friend?")
        print("æ£€ç´¢ç»“æœï¼š")
        for i, doc in enumerate(results, 1):
            print(f"{i}. {doc.page_content} (source: {doc.metadata.get('source')})")
    except Exception as e:
        print(f"âŒ Chroma æµ‹è¯•å¤±è´¥: {e}")
